{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yolo3_Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8pPEV4qwqdU",
        "colab_type": "code",
        "outputId": "4b326bcd-7485-4ff0-8854-967a83e0a286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPDVMLZuAUHm",
        "colab_type": "code",
        "outputId": "f8aa5969-7b8a-4c7c-b285-672eacaa22c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Q10_KNDDjX",
        "colab_type": "code",
        "outputId": "09e7b654-33ee-4c05-ad03-e5488039b3cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/keras-yolo3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/keras-yolo3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aclreoIbQf6",
        "colab_type": "code",
        "outputId": "a0357673-dc69-43c9-e45b-e8ebd41418b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from utils.bbox import BoundBox, bbox_iou\n",
        "from utils.image import apply_random_scale_and_crop, random_distort_image, random_flip, correct_bounding_boxes\n",
        "from yolo import create_yolov3_model, dummy_loss\n",
        "from utils.utils import normalize, evaluate, makedirs\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from callbacks import CustomModelCheckpoint, CustomTensorBoard"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOUuXIgpEw9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import package for preparing the dataset\n",
        "import numpy as np \n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import json\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTPQGALwFCKv",
        "colab_type": "text"
      },
      "source": [
        "## Preparing Datatset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRPdUcuu2YXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "def parse_xml(ann_dir, img_dir, cache_name, labels=[]):\n",
        "    if os.path.exists(cache_name):\n",
        "        with open(cache_name, 'rb') as handle:\n",
        "            cache = pickle.load(handle)\n",
        "        all_insts, seen_labels = cache['all_insts'], cache['seen_labels']\n",
        "    else:\n",
        "        all_insts = []\n",
        "        seen_labels = {}\n",
        "        \n",
        "        for ann in sorted(os.listdir(ann_dir)):\n",
        "            img = {'object':[]}\n",
        "\n",
        "            try:\n",
        "                tree = ET.parse(ann_dir + ann)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print('Ignore this bad annotation: ' + ann_dir + ann)\n",
        "                continue\n",
        "            \n",
        "            for elem in tree.iter():\n",
        "                if 'filename' in elem.tag:\n",
        "                    img['filename'] = img_dir + elem.text\n",
        "                if 'width' in elem.tag:\n",
        "                    img['width'] = int(elem.text)\n",
        "                if 'height' in elem.tag:\n",
        "                    img['height'] = int(elem.text)\n",
        "                if 'object' in elem.tag or 'part' in elem.tag:\n",
        "                    obj = {}\n",
        "                    \n",
        "                    for attr in list(elem):\n",
        "                        if 'name' in attr.tag:\n",
        "                            obj['name'] = attr.text\n",
        "\n",
        "                            if obj['name'] in seen_labels:\n",
        "                                seen_labels[obj['name']] += 1\n",
        "                            else:\n",
        "                                seen_labels[obj['name']] = 1\n",
        "                            \n",
        "                            if len(labels) > 0 and obj['name'] not in labels:\n",
        "                                break\n",
        "                            else:\n",
        "                                img['object'] += [obj]\n",
        "                                \n",
        "                        if 'bndbox' in attr.tag:\n",
        "                            for dim in list(attr):\n",
        "                                if 'xmin' in dim.tag:\n",
        "                                    obj['xmin'] = int(round(float(dim.text)))\n",
        "                                if 'ymin' in dim.tag:\n",
        "                                    obj['ymin'] = int(round(float(dim.text)))\n",
        "                                if 'xmax' in dim.tag:\n",
        "                                    obj['xmax'] = int(round(float(dim.text)))\n",
        "                                if 'ymax' in dim.tag:\n",
        "                                    obj['ymax'] = int(round(float(dim.text)))\n",
        "\n",
        "            if len(img['object']) > 0:\n",
        "                all_insts += [img]\n",
        "\n",
        "        cache = {'all_insts': all_insts, 'seen_labels': seen_labels}\n",
        "        with open(cache_name, 'wb') as handle:\n",
        "            pickle.dump(cache, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n",
        "                        \n",
        "    return all_insts, seen_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRtHJks32dvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_training_instance(train_anno_folder, train_image_folder, train_cache, valid_anno_folder, valid_image_folder, valid_cache, labels):\n",
        "  train_instance, train_labels = parse_xml(train_anno_folder, train_image_folder, train_cache, labels)\n",
        "  valid_instance, valid_labels = parse_xml(valid_anno_folder, valid_image_folder, valid_cache, labels)\n",
        "  labels = train_labels.keys()\n",
        "  \n",
        "  max_box = max([len(instance['object']) for instance in (train_instance+valid_instance)])\n",
        "  print(max_box)\n",
        "  return train_instance, valid_instance, sorted(labels), max_box"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SxGGtEX37v2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_callbacks(saved_weights_name, tensorboard_logs, model_to_save):\n",
        "    makedirs(tensorboard_logs)\n",
        "    \n",
        "    early_stop = EarlyStopping(\n",
        "        monitor     = 'loss', \n",
        "        min_delta   = 0.01, \n",
        "        patience    = 5, \n",
        "        mode        = 'min', \n",
        "        verbose     = 1\n",
        "    )\n",
        "    checkpoint = CustomModelCheckpoint(\n",
        "        model_to_save   = model_to_save,\n",
        "        filepath        = saved_weights_name,# + '{epoch:02d}.h5', \n",
        "        monitor         = 'loss', \n",
        "        verbose         = 1, \n",
        "        save_best_only  = True, \n",
        "        mode            = 'min', \n",
        "        period          = 1\n",
        "    )\n",
        "    reduce_on_plateau = ReduceLROnPlateau(\n",
        "        monitor  = 'loss',\n",
        "        factor   = 0.1,\n",
        "        patience = 2,\n",
        "        verbose  = 1,\n",
        "        mode     = 'min',\n",
        "        epsilon  = 0.01,\n",
        "        cooldown = 0,\n",
        "        min_lr   = 0\n",
        "    )\n",
        "    tensorboard = CustomTensorBoard(\n",
        "        log_dir                = tensorboard_logs,\n",
        "        write_graph            = True,\n",
        "        write_images           = True,\n",
        "    )    \n",
        "    return [early_stop, checkpoint, reduce_on_plateau, tensorboard]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGoLQhUn4QKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(nb_class, anchors, max_box, max_grid, batch_size, warmup_batches, ignore_thresh, saved_weights, lr, grid_scales, obj_scale, noobj_scale, xywh_scale, class_scale):\n",
        "  train_model, infer_model = create_yolov3_model(\n",
        "              nb_class            = nb_class, \n",
        "              anchors             = anchors, \n",
        "              max_box_per_image   = max_box, \n",
        "              max_grid            = max_grid, \n",
        "              batch_size          = batch_size, \n",
        "              warmup_batches      = warmup_batches,\n",
        "              ignore_thresh       = ignore_thresh,\n",
        "              grid_scales         = grid_scales,\n",
        "              obj_scale           = obj_scale,\n",
        "              noobj_scale         = noobj_scale,\n",
        "              xywh_scale          = xywh_scale,\n",
        "              class_scale         = class_scale\n",
        "              )\n",
        "  if os.path.exists(saved_weights):\n",
        "    train_model.load_weights(saved_weights)\n",
        "\n",
        "\n",
        "  train_model.compile(loss=dummy_loss, optimizer= Adam(lr=lr))\n",
        "  return train_model, infer_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCLUiIT08QaW",
        "colab_type": "code",
        "outputId": "cde5394c-69cf-4c52-ca98-6848c40216d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('config_rbc_new.json') as config:\n",
        "  config = json.loads(config.read())\n",
        "\n",
        "#Parse Annotation \n",
        "train_instance, valid_instance, labels, max_box = create_training_instance(\n",
        "    config['train']['train_annot_folder'],\n",
        "    config['train']['train_image_folder'],\n",
        "    config['train']['cache_name'],\n",
        "    config['valid']['valid_annot_folder'],\n",
        "    config['valid']['valid_image_folder'],\n",
        "    config['valid']['cache_name'],\n",
        "    config['model']['labels']\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJY9XU5EePgX",
        "colab_type": "text"
      },
      "source": [
        "## Define BatchGenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQupj6rSXgBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import Sequence\n",
        "#define batch generator \n",
        "class BatchGenerator(Sequence):\n",
        "    def __init__(self, \n",
        "        instances, \n",
        "        anchors,   \n",
        "        labels,        \n",
        "        downsample=32, # ratio between network input's size and network output's size, 32 for YOLOv3\n",
        "        max_box_per_image=30,\n",
        "        batch_size=1,\n",
        "        min_net_size=320,\n",
        "        max_net_size=608,    \n",
        "        shuffle=True, \n",
        "        jitter=True, \n",
        "        norm=None\n",
        "    ):\n",
        "        self.instances          = instances\n",
        "        self.batch_size         = batch_size\n",
        "        self.labels             = labels\n",
        "        self.downsample         = downsample\n",
        "        self.max_box_per_image  = max_box_per_image\n",
        "        self.min_net_size       = (min_net_size//self.downsample)*self.downsample\n",
        "        self.max_net_size       = (max_net_size//self.downsample)*self.downsample\n",
        "        self.shuffle            = shuffle\n",
        "        self.jitter             = jitter\n",
        "        self.norm               = norm\n",
        "        self.anchors            = [BoundBox(0, 0, anchors[2*i], anchors[2*i+1]) for i in range(len(anchors)//2)]\n",
        "        self.net_h              = 416  \n",
        "        self.net_w              = 416\n",
        "\n",
        "        if shuffle: np.random.shuffle(self.instances)\n",
        "            \n",
        "    def __len__(self):\n",
        "        return int(np.ceil(float(len(self.instances))/self.batch_size))           \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get image input size, change every 10 batches\n",
        "        net_h, net_w = self._get_net_size(idx)\n",
        "        base_grid_h, base_grid_w = net_h//self.downsample, net_w//self.downsample\n",
        "\n",
        "        # determine the first and the last indices of the batch\n",
        "        l_bound = idx*self.batch_size\n",
        "        r_bound = (idx+1)*self.batch_size\n",
        "\n",
        "        if r_bound > len(self.instances):\n",
        "            r_bound = len(self.instances)\n",
        "            l_bound = r_bound - self.batch_size\n",
        "\n",
        "        x_batch = np.zeros((r_bound - l_bound, net_h, net_w, 3))             # input images\n",
        "        t_batch = np.zeros((r_bound - l_bound, 1, 1, 1,  self.max_box_per_image, 4))   # list of groundtruth boxes\n",
        "\n",
        "        # initialize the inputs and the outputs\n",
        "        yolo_1 = np.zeros((r_bound - l_bound, 1*base_grid_h,  1*base_grid_w, len(self.anchors)//3, 4+1+len(self.labels))) # desired network output 1\n",
        "        yolo_2 = np.zeros((r_bound - l_bound, 2*base_grid_h,  2*base_grid_w, len(self.anchors)//3, 4+1+len(self.labels))) # desired network output 2\n",
        "        yolo_3 = np.zeros((r_bound - l_bound, 4*base_grid_h,  4*base_grid_w, len(self.anchors)//3, 4+1+len(self.labels))) # desired network output 3\n",
        "        yolos = [yolo_3, yolo_2, yolo_1]\n",
        "\n",
        "        dummy_yolo_1 = np.zeros((r_bound - l_bound, 1))\n",
        "        dummy_yolo_2 = np.zeros((r_bound - l_bound, 1))\n",
        "        dummy_yolo_3 = np.zeros((r_bound - l_bound, 1))\n",
        "        \n",
        "        instance_count = 0\n",
        "        true_box_index = 0\n",
        "\n",
        "        # do the logic to fill in the inputs and the output\n",
        "        for train_instance in self.instances[l_bound:r_bound]:\n",
        "            # augment input image and fix object's position and size\n",
        "            img, all_objs = self._aug_image(train_instance, net_h, net_w)\n",
        "            \n",
        "            for obj in all_objs:\n",
        "                # find the best anchor box for this object\n",
        "                max_anchor = None                \n",
        "                max_index  = -1\n",
        "                max_iou    = -1\n",
        "\n",
        "                shifted_box = BoundBox(0, \n",
        "                                       0,\n",
        "                                       obj['xmax']-obj['xmin'],                                                \n",
        "                                       obj['ymax']-obj['ymin'])    \n",
        "                \n",
        "                for i in range(len(self.anchors)):\n",
        "                    anchor = self.anchors[i]\n",
        "                    iou    = bbox_iou(shifted_box, anchor)\n",
        "\n",
        "                    if max_iou < iou:\n",
        "                        max_anchor = anchor\n",
        "                        max_index  = i\n",
        "                        max_iou    = iou                \n",
        "                \n",
        "                # determine the yolo to be responsible for this bounding box\n",
        "                yolo = yolos[max_index//3]\n",
        "                grid_h, grid_w = yolo.shape[1:3]\n",
        "                \n",
        "                # determine the position of the bounding box on the grid\n",
        "                center_x = .5*(obj['xmin'] + obj['xmax'])\n",
        "                center_x = center_x / float(net_w) * grid_w # sigma(t_x) + c_x\n",
        "                center_y = .5*(obj['ymin'] + obj['ymax'])\n",
        "                center_y = center_y / float(net_h) * grid_h # sigma(t_y) + c_y\n",
        "                \n",
        "                # determine the sizes of the bounding box\n",
        "                w = np.log((obj['xmax'] - obj['xmin']) / float(max_anchor.xmax)) # t_w\n",
        "                h = np.log((obj['ymax'] - obj['ymin']) / float(max_anchor.ymax)) # t_h\n",
        "\n",
        "                box = [center_x, center_y, w, h]\n",
        "\n",
        "                # determine the index of the label\n",
        "                obj_indx = self.labels.index(obj['name'])  \n",
        "\n",
        "                # determine the location of the cell responsible for this object\n",
        "                grid_x = int(np.floor(center_x))\n",
        "                grid_y = int(np.floor(center_y))\n",
        "\n",
        "                # assign ground truth x, y, w, h, confidence and class probs to y_batch\n",
        "                yolo[instance_count, grid_y, grid_x, max_index%3]      = 0\n",
        "                yolo[instance_count, grid_y, grid_x, max_index%3, 0:4] = box\n",
        "                yolo[instance_count, grid_y, grid_x, max_index%3, 4  ] = 1.\n",
        "                yolo[instance_count, grid_y, grid_x, max_index%3, 5+obj_indx] = 1\n",
        "\n",
        "                # assign the true box to t_batch\n",
        "                true_box = [center_x, center_y, obj['xmax'] - obj['xmin'], obj['ymax'] - obj['ymin']]\n",
        "                t_batch[instance_count, 0, 0, 0, true_box_index] = true_box\n",
        "\n",
        "                true_box_index += 1\n",
        "                true_box_index  = true_box_index % self.max_box_per_image    \n",
        "\n",
        "            # assign input image to x_batch\n",
        "            if self.norm != None: \n",
        "                x_batch[instance_count] = self.norm(img)\n",
        "            else:\n",
        "                # plot image and bounding boxes for sanity check\n",
        "                for obj in all_objs:\n",
        "                    cv2.rectangle(img, (obj['xmin'],obj['ymin']), (obj['xmax'],obj['ymax']), (255,0,0), 3)\n",
        "                    cv2.putText(img, obj['name'], \n",
        "                                (obj['xmin']+2, obj['ymin']+12), \n",
        "                                0, 1.2e-3 * img.shape[0], \n",
        "                                (0,255,0), 2)\n",
        "                \n",
        "                x_batch[instance_count] = img\n",
        "\n",
        "            # increase instance counter in the current batch\n",
        "            instance_count += 1                 \n",
        "                \n",
        "        return [x_batch, t_batch, yolo_1, yolo_2, yolo_3], [dummy_yolo_1, dummy_yolo_2, dummy_yolo_3]\n",
        "\n",
        "    def _get_net_size(self, idx):\n",
        "        if idx%10 == 0:\n",
        "            net_size = self.downsample*np.random.randint(self.min_net_size/self.downsample, \\\n",
        "                                                         self.max_net_size/self.downsample+1)\n",
        "            print(\"resizing: \", net_size, net_size)\n",
        "            self.net_h, self.net_w = net_size, net_size\n",
        "        return self.net_h, self.net_w\n",
        "    \n",
        "    def _aug_image(self, instance, net_h, net_w):\n",
        "        image_name = instance['filename']\n",
        "        image = cv2.imread(image_name) # RGB image\n",
        "        \n",
        "        if image is None: print('Cannot find ', image_name)\n",
        "        image = image[:,:,::-1] # RGB image\n",
        "            \n",
        "        image_h, image_w, _ = image.shape\n",
        "        \n",
        "        # determine the amount of scaling and cropping\n",
        "        dw = self.jitter * image_w;\n",
        "        dh = self.jitter * image_h;\n",
        "\n",
        "        new_ar = (image_w + np.random.uniform(-dw, dw)) / (image_h + np.random.uniform(-dh, dh));\n",
        "        scale = np.random.uniform(0.25, 2);\n",
        "\n",
        "        if (new_ar < 1):\n",
        "            new_h = int(scale * net_h);\n",
        "            new_w = int(net_h * new_ar);\n",
        "        else:\n",
        "            new_w = int(scale * net_w);\n",
        "            new_h = int(net_w / new_ar);\n",
        "            \n",
        "        dx = int(np.random.uniform(0, net_w - new_w));\n",
        "        dy = int(np.random.uniform(0, net_h - new_h));\n",
        "        \n",
        "        # apply scaling and cropping\n",
        "        im_sized = apply_random_scale_and_crop(image, new_w, new_h, net_w, net_h, dx, dy)\n",
        "        \n",
        "        # randomly distort hsv space\n",
        "        im_sized = random_distort_image(im_sized)\n",
        "        \n",
        "        # randomly flip\n",
        "        flip = np.random.randint(2)\n",
        "        im_sized = random_flip(im_sized, flip)\n",
        "            \n",
        "        # correct the size and pos of bounding boxes\n",
        "        all_objs = correct_bounding_boxes(instance['object'], new_w, new_h, net_w, net_h, dx, dy, flip, image_w, image_h)\n",
        "        \n",
        "        return im_sized, all_objs   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzYk1J5AJ-OD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(image):\n",
        "  return image/255\n",
        "\n",
        "##create generators\n",
        "train_generator = BatchGenerator(\n",
        "        instances           = train_instance, \n",
        "        anchors             = config['model']['anchors'],   \n",
        "        labels              = labels,        \n",
        "        downsample          = 32, # ratio between network input's size and network output's size, 32 for YOLOv3\n",
        "        max_box_per_image   = max_box,\n",
        "        batch_size          = config['train']['batch_size'],\n",
        "        min_net_size        = config['model']['min_input_size'],\n",
        "        max_net_size        = config['model']['max_input_size'],   \n",
        "        shuffle             = True, \n",
        "        jitter              = 0.3, \n",
        "        norm                = normalize\n",
        ")\n",
        "  \n",
        "\n",
        "valid_generator = BatchGenerator(\n",
        "        instances           = valid_instance, \n",
        "        anchors             = config['model']['anchors'],   \n",
        "        labels              = labels,        \n",
        "        downsample          = 32, # ratio between network input's size and network output's size, 32 for YOLOv3\n",
        "        max_box_per_image   = max_box,\n",
        "        batch_size          = config['train']['batch_size'],\n",
        "        min_net_size        = config['model']['min_input_size'],\n",
        "        max_net_size        = config['model']['max_input_size'],   \n",
        "        shuffle             = True, \n",
        "        jitter              = 0.0, \n",
        "        norm                = normalize\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqZyAtkDXQC5",
        "colab_type": "code",
        "outputId": "acee3a07-e7b9-4f3e-a688-036ae44d3203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#create model \n",
        "if os.path.exists(config['train']['saved_weights_name']): \n",
        "  config['train']['warmup_epochs'] = 0\n",
        "warmup_batches = config['train']['warmup_epochs'] * (config['train']['train_times']*len(train_generator))   \n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = config['train']['gpus']\n",
        "\n",
        "train_model, infer_model = create_model(\n",
        "      nb_class = len(labels),\n",
        "      anchors = config['model']['anchors'],\n",
        "      max_box = max_box,\n",
        "      max_grid = [config['model']['max_input_size'], config['model']['max_input_size']], \n",
        "      batch_size = config['train']['batch_size'], \n",
        "      warmup_batches      = warmup_batches,\n",
        "      ignore_thresh       = config['train']['ignore_thresh'],\n",
        "      saved_weights  = config['train']['saved_weights_name'],\n",
        "      lr                  = config['train']['learning_rate'],\n",
        "      grid_scales         = config['train']['grid_scales'],\n",
        "      obj_scale           = config['train']['obj_scale'],\n",
        "      noobj_scale         = config['train']['noobj_scale'],\n",
        "      xywh_scale          = config['train']['xywh_scale'],\n",
        "      class_scale         = config['train']['class_scale'],\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81qT4iAq6BbF",
        "colab_type": "code",
        "outputId": "c59d34f2-c49d-4651-b754-0b7e16d091bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "callbacks = create_callbacks(config['train']['saved_weights_name'], config['train']['tensorboard_dir'], infer_model)\n",
        "\n",
        "train_model.fit_generator(\n",
        "        generator        = train_generator, \n",
        "        steps_per_epoch  = len(train_generator) * config['train']['train_times'], \n",
        "        epochs           = config['train']['nb_epochs'] + config['train']['warmup_epochs'], \n",
        "        verbose          = 2 if config['train']['debug'] else 1,\n",
        "        callbacks        = callbacks, \n",
        "#        workers          = 4,\n",
        "#        max_queue_size   = 8\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
            "  warnings.warn('`epsilon` argument is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "resizing:  288 288\n",
            "resizing:  288 288\n",
            "resizing:  288 288\n",
            "resizing:  288 288\n",
            "resizing:  288 288\n",
            "resizing:  416 416\n",
            "resizing:  288 288\n",
            "resizing:  416 416\n",
            "resizing:  416 416\n",
            "resizing:  320 320\n",
            "resizing:  256 256\n",
            "resizing:  224 224\n",
            "resizing:  256 256\n",
            "resizing:  288 288\n",
            "resizing:  352 352\n",
            "resizing:  288 288\n",
            "resizing:  480 480\n",
            "resizing:  288 288\n",
            "resizing:  288 288\n",
            "resizing:  320 320\n",
            "resizing:  416 416\n",
            "resizing:  448 448\n",
            "resizing:  224 224\n",
            "resizing:  320 320\n",
            "resizing:  320 320\n",
            "resizing:  288 288\n",
            " - 169s - loss: 57.7360 - yolo_layer_4_loss: 7.8771 - yolo_layer_5_loss: 15.5947 - yolo_layer_6_loss: 34.2642\n",
            "\n",
            "Epoch 00001: loss improved from inf to 57.73597, saving model to darknet53.h5\n",
            "Epoch 2/10\n",
            "resizing:  352 352\n",
            "resizing:  256 256\n",
            "resizing:  480 480\n",
            "resizing:  416 416\n",
            "resizing:  288 288\n",
            "resizing:  384 384\n",
            "resizing:  352 352\n",
            "resizing:  352 352\n",
            "resizing:  416 416\n",
            "resizing:  448 448\n",
            "resizing:  416 416\n",
            "resizing:  352 352\n",
            "resizing:  448 448\n",
            "resizing:  288 288\n",
            "resizing:  288 288\n",
            "resizing:  384 384\n",
            "resizing:  352 352\n",
            "resizing:  320 320\n",
            "resizing:  224 224\n",
            "resizing:  224 224\n",
            "resizing:  384 384\n",
            "resizing:  416 416\n",
            " - 155s - loss: 46.9747 - yolo_layer_4_loss: 6.5790 - yolo_layer_5_loss: 13.5034 - yolo_layer_6_loss: 26.8923\n",
            "\n",
            "Epoch 00002: loss improved from 57.73597 to 46.97470, saving model to darknet53.h5\n",
            "Epoch 3/10\n",
            "resizing:  416 416\n",
            "resizing:  320 320\n",
            "resizing:  352 352\n",
            "resizing:  352 352\n",
            "resizing:  352 352\n",
            "resizing:  352 352\n",
            "resizing:  416 416\n",
            "resizing:  224 224\n",
            "resizing:  384 384\n",
            "resizing:  416 416\n",
            "resizing:  416 416\n",
            "resizing:  224 224\n",
            "resizing:  288 288\n",
            "resizing:  256 256\n",
            "resizing:  288 288\n",
            "resizing:  288 288\n",
            "resizing:  384 384\n",
            "resizing:  224 224\n",
            "resizing:  448 448\n",
            "resizing:  224 224\n",
            "resizing:  224 224\n",
            "resizing:  480 480\n",
            "resizing:  416 416\n",
            "resizing:  352 352\n",
            "resizing:  224 224\n",
            "resizing:  448 448\n",
            " - 156s - loss: 41.8953 - yolo_layer_4_loss: 6.0538 - yolo_layer_5_loss: 11.6392 - yolo_layer_6_loss: 24.2023\n",
            "\n",
            "Epoch 00003: loss improved from 46.97470 to 41.89535, saving model to darknet53.h5\n",
            "Epoch 4/10\n",
            "resizing:  416 416\n",
            "resizing:  320 320\n",
            "resizing:  416 416\n",
            "resizing:  320 320\n",
            "resizing:  448 448\n",
            "resizing:  416 416\n",
            "resizing:  320 320\n",
            "resizing:  448 448\n",
            "resizing:  448 448\n",
            "resizing:  416 416\n",
            "resizing:  224 224\n",
            "resizing:  256 256\n",
            "resizing:  320 320\n",
            "resizing:  256 256\n",
            "resizing:  256 256\n",
            "resizing:  288 288\n",
            "resizing:  352 352\n",
            "resizing:  448 448\n",
            "resizing:  256 256\n",
            "resizing:  224 224\n",
            "resizing:  224 224\n",
            "resizing:  416 416\n",
            " - 161s - loss: 37.2798 - yolo_layer_4_loss: 5.4636 - yolo_layer_5_loss: 10.8335 - yolo_layer_6_loss: 20.9828\n",
            "\n",
            "Epoch 00004: loss improved from 41.89535 to 37.27985, saving model to darknet53.h5\n",
            "resizing:  416 416\n",
            "resizing:  448 448\n",
            "Epoch 5/10\n",
            "resizing:  480 480\n",
            "resizing:  384 384\n",
            "resizing:  256 256\n",
            "resizing:  448 448\n",
            "resizing:  384 384\n",
            "resizing:  320 320\n",
            "resizing:  480 480\n",
            "resizing:  384 384\n",
            "resizing:  288 288\n",
            "resizing:  448 448\n",
            "resizing:  416 416\n",
            "resizing:  448 448\n",
            "resizing:  320 320\n",
            "resizing:  288 288\n",
            "resizing:  416 416\n",
            "resizing:  352 352\n",
            "resizing:  416 416\n",
            "resizing:  320 320\n",
            "resizing:  480 480\n",
            "resizing:  224 224\n",
            "resizing:  320 320\n",
            "resizing:  288 288\n",
            "resizing:  416 416\n",
            "resizing:  384 384\n",
            " - 172s - loss: 35.4204 - yolo_layer_4_loss: 5.7042 - yolo_layer_5_loss: 10.3023 - yolo_layer_6_loss: 19.4139\n",
            "\n",
            "Epoch 00005: loss improved from 37.27985 to 35.42041, saving model to darknet53.h5\n",
            "Epoch 6/10\n",
            "resizing:  352 352\n",
            "resizing:  288 288\n",
            "resizing:  352 352\n",
            "resizing:  320 320\n",
            "resizing:  256 256\n",
            "resizing:  320 320\n",
            "resizing:  480 480\n",
            "resizing:  352 352\n",
            "resizing:  416 416\n",
            "resizing:  320 320\n",
            "resizing:  384 384\n",
            "resizing:  416 416\n",
            "resizing:  288 288\n",
            "resizing:  352 352\n",
            "resizing:  448 448\n",
            "resizing:  480 480\n",
            "resizing:  384 384\n",
            "resizing:  320 320\n",
            "resizing:  288 288\n",
            "resizing:  448 448\n",
            "resizing:  288 288\n",
            "resizing:  352 352\n",
            "resizing:  384 384\n",
            "resizing:  384 384\n",
            " - 163s - loss: 32.3754 - yolo_layer_4_loss: 4.6964 - yolo_layer_5_loss: 9.4903 - yolo_layer_6_loss: 18.1886\n",
            "\n",
            "Epoch 00006: loss improved from 35.42041 to 32.37539, saving model to darknet53.h5\n",
            "resizing:  416 416\n",
            "Epoch 7/10\n",
            "resizing:  288 288\n",
            "resizing:  224 224\n",
            "resizing:  288 288\n",
            "resizing:  224 224\n",
            "resizing:  224 224\n",
            "resizing:  480 480\n",
            "resizing:  352 352\n",
            "resizing:  480 480\n",
            "resizing:  352 352\n",
            "resizing:  384 384\n",
            "resizing:  224 224\n",
            "resizing:  480 480\n",
            "resizing:  352 352\n",
            "resizing:  352 352\n",
            "resizing:  352 352\n",
            "resizing:  416 416\n",
            "resizing:  288 288\n",
            "resizing:  352 352\n",
            "resizing:  256 256\n",
            "resizing:  352 352\n",
            "resizing:  224 224\n",
            "resizing:  384 384\n",
            " - 144s - loss: 30.2636 - yolo_layer_4_loss: 3.5299 - yolo_layer_5_loss: 7.7413 - yolo_layer_6_loss: 18.9924\n",
            "\n",
            "Epoch 00007: loss improved from 32.37539 to 30.26360, saving model to darknet53.h5\n",
            "resizing:  480 480\n",
            "Epoch 8/10\n",
            "resizing:  384 384\n",
            "resizing:  256 256\n",
            "resizing:  448 448\n",
            "resizing:  448 448\n",
            "resizing:  480 480\n",
            "resizing:  256 256\n",
            "resizing:  384 384\n",
            "resizing:  384 384\n",
            "resizing:  288 288\n",
            "resizing:  448 448\n",
            "resizing:  288 288\n",
            "resizing:  448 448\n",
            "resizing:  416 416\n",
            "resizing:  448 448\n",
            "resizing:  256 256\n",
            "resizing:  416 416\n",
            "resizing:  480 480\n",
            "resizing:  224 224\n",
            "resizing:  288 288\n",
            "resizing:  256 256\n",
            "resizing:  480 480\n",
            "resizing:  416 416\n",
            "resizing:  288 288\n",
            " - 164s - loss: 32.0020 - yolo_layer_4_loss: 4.9220 - yolo_layer_5_loss: 8.9238 - yolo_layer_6_loss: 18.1562\n",
            "\n",
            "Epoch 00008: loss did not improve from 30.26360\n",
            "Epoch 9/10\n",
            "resizing:  320 320\n",
            "resizing:  224 224\n",
            "resizing:  480 480\n",
            "resizing:  384 384\n",
            "resizing:  224 224\n",
            "resizing:  480 480\n",
            "resizing:  480 480\n",
            "resizing:  288 288\n",
            "resizing:  256 256\n",
            "resizing:  224 224\n",
            "resizing:  256 256\n",
            "resizing:  256 256\n",
            "resizing:  256 256\n",
            "resizing:  320 320\n",
            "resizing:  288 288\n",
            "resizing:  448 448\n",
            "resizing:  352 352\n",
            "resizing:  288 288\n",
            "resizing:  288 288\n",
            "resizing:  288 288\n",
            "resizing:  416 416\n",
            "resizing:  416 416\n",
            "resizing:  480 480\n",
            "resizing:  288 288\n",
            " - 152s - loss: 30.5763 - yolo_layer_4_loss: 4.3637 - yolo_layer_5_loss: 8.1406 - yolo_layer_6_loss: 18.0720\n",
            "\n",
            "Epoch 00009: loss did not improve from 30.26360\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "Epoch 10/10\n",
            "resizing:  384 384\n",
            "resizing:  416 416\n",
            "resizing:  320 320\n",
            "resizing:  416 416\n",
            "resizing:  224 224\n",
            "resizing:  256 256\n",
            "resizing:  352 352\n",
            "resizing:  384 384\n",
            "resizing:  416 416\n",
            "resizing:  256 256\n",
            "resizing:  448 448\n",
            "resizing:  480 480\n",
            "resizing:  480 480\n",
            "resizing:  480 480\n",
            "resizing:  416 416\n",
            "resizing:  288 288\n",
            "resizing:  448 448\n",
            "resizing:  256 256\n",
            "resizing:  480 480\n",
            "resizing:  416 416\n",
            "resizing:  224 224\n",
            "resizing:  320 320\n",
            "resizing:  320 320\n",
            "resizing:  480 480\n",
            " - 164s - loss: 29.8307 - yolo_layer_4_loss: 4.0735 - yolo_layer_5_loss: 8.4490 - yolo_layer_6_loss: 17.3082\n",
            "\n",
            "Epoch 00010: loss improved from 30.26360 to 29.83067, saving model to darknet53.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f81d2c010f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}